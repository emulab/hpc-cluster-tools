Source: http://www.sdsc.edu/~hocks/FG/PBS.slurm.html


Comparison between SLURM and Torque (PBS)

torque queue = slurm partition


Comparison of some common commands in SLURM and in Torque (PBS) and Maui. 


Task  	 			Torque/PBS  			SLURM
-------------------------------------------------------------------------
Submit a job  			qsub myjob.sh  			sbatch myjob.sh
Delete a job  			qdel 123  			scancel 123
Show job status  		qstat  				squeue
Show expected job start time  - (showstart in Maui/Moab)  	squeue --start
Show queue info  		qstat -q  			sinfo
Show job details  		qstat -f 123  			scontrol show job 123
Show queue details  		qstat -Q -f   		scontrol show partition 
Show node details  		pbsnode n0000  			scontrol show node n0000
Show QoS details  - 		(mdiag -q  in Maui/Moab)	sacctmgr show qos 


Job control 



Action                          Slurm                           Torque/PBS              Maui		SGE
------------------------------------------------------------------------------------------------------------
Get information about the job   scontrol show job "jobid"	qstat -f "jobid"        checkjob	
Display the queue info		squeue                          qstat                   showq		qstat
Delete a job                    scancel "jobid"                 qdel   					qdel
Clean up leftover job						momctl -c JobID
Submit a job                    srun/sbatch/salloc testjob      qsub testjob		msub		qsub
Submit a interactive job	salloc -N 4 -p active sh	qsub -I					qlogin
Display all job info		squeue -al			qstat -f 
				scontrol show job
Display job  			scontrol show job "jobID"	qstat -f "jobID"
Display free processors 	srun --test-only -p normal \
				  -n 1 -t 10:00 sh					showbf 
Display the expected start time squeue --start -j "jobid" 				showstart "jobid"
Display blocked jobs		squeue --start						mdiag -b/showq -b
Display queues/partitions	scontrol show partition		qstat -Qf 		mdiag -c
Display queue			sinfo -h			qstat -q		
				sinfo -o "%P %l %c %D "		

Start job                       scontrol update JobId=1234 \    qrun  			runjob
                                  StartTime=now
Hold job                        scontrol update JobId=1234 \    qhold 			sethold
                                  StartTime=now+30days
Release hold job		scontrol update JobId=1234 \    qrls	                releasehold
                                  StartTime=now

Pending job         		scontrol requeue 1234
Graphical Frontend		sview				xpbs					qmon
set priority			scontrol update JobId=592 \				setspri 10000 592
				  nice=-10000

Node control



Action                          Slurm                               Torque/PBS              Maui
------------------------------------------------------------------------------------------------

Display node info		scontrol show node "node" 	     pbsnodes "node"	    checknode "node"
list feature and resource       sinfo -o "%15N %10c %10m  %25f %10G" pbsnodes 
Drain node			scontrol update NodeName=gpu-1-4     pbsnodes -oN "Timeout" tcc-1-4
				  State=DRAIN Reason=Timeout
Clear node			scontrol update NodeName=gpu-1-4     pbsnodes -cN "" tcc-1-4
				  State=RESUME
List down Nodes			sinfo --list-reasons		     pbsnodes -ln
				sinfo --long -R
				sinfo -RlN

list reservation		scontrol show res					    showres	
set reservation			scontrol create res starttime=now duration=04:00:00 Nodecnt=1 Users=hocks Flags=IGNORE

					setres -n tideker -s 00:00:01_06/11/2014 -d 24:00:00 -f tideker-node TASKS==256


pbsnodes -l all:	
	alias pn='sinfo --format="%25N %.3D %9P %11T %.4c %14C %.8z %.8m %.4d %.8w %10f %20E"'

pbsnodes -ln
	alias pl='sinfo --states=down,drain,fail,no_respond,maint,unk --format="%12n %20f %20H %12u %32E"'

showstart: 
	alias lj='sacct -o user,jobid,jobname,state,node,start'


list allocated/idle/other/total:
	sinfo --long -o %F



Configuration control



Action                          Slurm                           Torque/PBS              Maui
------------------------------------------------------------------------------------------------

show configuration		scontrol show config					showconfig 


Job State

       CA  CANCELLED       Job was explicitly cancelled by the user or system administrator.  The job may or
                           may not have been initiated.

       CD  COMPLETED       Job has terminated all processes on all nodes.

       CF  CONFIGURING     Job has been allocated resources, but are waiting for them to  become  ready  for
                           use (e.g. booting).

       CG  COMPLETING      Job  is  in  the process of completing. Some processes on some nodes may still be
                           active.

       F   FAILED          Job terminated with non-zero exit code or other failure condition.

       NF  NODE_FAIL       Job terminated due to failure of one or more allocated nodes.

       PD  PENDING         Job is awaiting resource allocation.

       PR  PREEMPTED       Job terminated due to preemption.

       R   RUNNING         Job currently has an allocation.

       S   SUSPENDED       Job has an allocation, but execution has been suspended.

       TO  TIMEOUT         Job terminated upon reaching its time limit.


format options
The field specifications available include:

              %a  State/availability of a partition

              %A  Number of nodes by state in  the  format  "allocated/idle".
                  Do  not use this with a node state option ("%t" or "%T") or
                  the different node states will be placed on separate lines.

              %c  Number of CPUs per node

              %d  Size of temporary disk space per node in megabytes

              %D  Number of nodes

              %f  Features associated with the nodes

              %F  Number   of   nodes   by   state   in   the  format  "allo-
                  cated/idle/other/total".  Do not use this with a node state
                  option  ("%t" or "%T") or the different node states will be
                  placed on separate lines.

              %g  Groups which may use the nodes

              %h  Jobs may share nodes, "yes", "no", or "force"

              %l  Maximum time for any job  in  the  format  "days-hours:min-
                  utes:seconds"

              %m  Size of memory per node in megabytes

              %N  List of node names

              %P  Partition name

              %r  Only user root may initiate jobs, "yes" or "no"

              %R  The  reason a node is unavailable (down, drained, or drain-
                  ing states)

              %s  Maximum job size in nodes

              %t  State of nodes, compact form

              %T  State of nodes, extended form

              %w  Scheduling weight of the nodes

              %.<*>
                  right justification of the field

              %<*>
                  size of field

Job variables

Environment Variable  	Torque/PBS  		SLURM
--------------------------------------------------------
JOB ID  		PBS_JOBID  		SLURM_JOB_ID / SLURM_JOBID
JOB NAME  		PBS_JOBNAME  		SLURM_JOB_NAME
NODE LIST  		PBS_NODELIST  		SLURM_JOB_NODELIST / SLURM_NODELIST
JOB SUBMIT DIRECTORY  	PBS_O_WORKDIR  		SLURM_SUBMIT_DIR
JOB ARRAY ID (INDEX)  	PBS_ARRAY_INDEX  	SLURM_ARRAY_TASK_ID
USER			PBS_USER		SLURM_JOB_USER
prolog
slurm:

BASIL_RESERVATION_ID    Basil reservation ID. Available on Cray XT/XE systems only. 
MPIRUN_PARTITION 	BlueGene partition name. Available on BlueGene systems only. 
SLURM_JOB_ACCOUNT	Account name used for the job. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_CONSTRAINTS2	Features required to run the job. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_DERIVED_EC	The highest exit code of all of the job steps. Available in EpilogSlurmctld only. 
SLURM_JOB_EXIT_CODE	The exit code of the job script (or salloc). Available in EpilogSlurmctld only. 
SLURM_JOB_GID		Group ID of the job's owner. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_GROUP		Group name of the job's owner. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_ID		Job ID. 
SLURM_JOB_NAME		Name of the job. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_NODELIST	Nodes assigned to job. A SLURM hostlist expression. "scontrol show hostnames" can be used to convert this to a list of individual host names. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_PARTITION	Partition that job runs in. Available in PrologSlurmctld and EpilogSlurmctld only. 
SLURM_JOB_UID		User ID of the job's owner. 
SLURM_JOB_USER		User name of the job's owner.

pbs:
"Prologue Args:"
 "Job ID: $1"
 "User ID: $2"
 "Group ID: $3"


"Epilogue Args:"
 "Job ID: $1"
 "User ID: $2"
 "Group ID: $3"
 "Job Name: $4"
 "Session ID: $5"
 "Resource List: $6"
 "Resources Used: $7"
 "Queue Name: $8"
 "Account String: $9"
PBS

PBS_VERSION=TORQUE-3.0.5
PBS_JOBNAME=STDIN
PBS_ENVIRONMENT=PBS_INTERACTIVE
PBS_O_WORKDIR=/home/hocks
PBS_TASKNUM=1
PBS_O_HOME=/home/hocks
PBS_WALLTIME=64800
PBS_GPUFILE=/var/spool/torque/aux//28288.tscc-mgr.localgpu
PBS_MOMPORT=15003
PBS_O_QUEUE=hotel
PBS_O_LOGNAME=hocks
PBS_O_LANG=en_US.iso885915
PBS_JOBCOOKIE=BCAC0808F8A2A5BA27581105BEF243F1
PBS_NODENUM=0
PBS_NUM_NODES=1
PBS_O_SHELL=/bin/bash
PBS_SERVER=tscc-mgr.local
PBS_JOBID=28288.tscc-mgr.local
PBS_O_HOST=tscc-mgr.sdsc.edu
PBS_VNODENUM=0
PBS_QUEUE=hotel
PBS_O_MAIL=/var/spool/mail/hocks
PBS_NP=1
PBS_NUM_PPN=1
PBS_NODEFILE=/var/spool/torque/aux//28288.tscc-mgr.local
PBS_O_PATH=/opt/openmpi/intel/ib/bin:/opt/intel/composer_xe_2013.1.117/bin/intel64:/opt/scar/bin:/opt/scar/sbin:/opt/
openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/eclipse:/opt/gang
lia/bin:/opt/ganglia/sbin:/opt/gold/bin:/opt/gold/sbin::/usr/java/latest/bin:/opt/maven/bin:/opt/maui/bin:/opt/torque
/bin:/opt/torque/sbin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/hocks/bin



slurm

The variables in the following table are set inside the job.
SLURM_JOBID  A unique job identifier assigned by SLURM
The jobname set by -N.

SLURM_JOB_NODELIST  	String containing a coded version of the list of nodes assigned to the job
SLURM_JOB_NUM_NODES  	The number of compute nodes assigned to the parallel job.
SLURM_NTASKS_PER_NODE  	The number of tasks to start per node
SLURM_NTASKS  		The total number of tasks available for the job


env:

SLURM_NODELIST=gpu-1-4
SLURM_NODE_ALIASES=(null)
SLURM_NNODES=1
SLURM_JOBID=48
SLURM_TASKS_PER_NODE=1
SLURM_JOB_ID=48
SLURM_SUBMIT_DIR=/etc/slurm
SLURM_JOB_NODELIST=gpu-1-4
SLURM_JOB_CPUS_PER_NODE=1
SLURM_SUBMIT_HOST=mskcc-fe1.sdsc.edu
SLURM_JOB_NUM_NODES=1
